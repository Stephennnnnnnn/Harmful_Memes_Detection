{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD9OoA5nnjbf",
        "outputId": "efb233ab-af95-4721-ea5d-50c369de9ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.1.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "!pip install ftfy\n",
        "\n",
        "# Authenticate\n",
        "drive = None\n",
        "def authenticate():\n",
        "    global drive\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "\n",
        "#Download files\n",
        "def downloadFiles(fileIds):\n",
        "    authenticate()\n",
        "    for fileId in fileIds:    \n",
        "        downloaded = drive.CreateFile({\"id\": fileId[1]})\n",
        "        downloaded.GetContentFile(fileId[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download file if not existing\n",
        "try:\n",
        "  _ = open(\"bpe_simple_vocab_16e6.txt.gz\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"bpe_simple_vocab_16e6.txt.gz\", \"1sazrmZm-bsAyLap-kvFCVn8uXdGppVo-\"]])\n",
        "\n",
        "try:\n",
        "  _ = open(\"clip_model.pt\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"clip_model.pt\", \"1UO7E2nu_4-C5RRcTtYKqCRjLPUtzV2NQ\"]])\n",
        "\n"
      ],
      "metadata": {
        "id": "8VXH9xYxnt-8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "# from torchnlp import encoders\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import  mean_absolute_error\n",
        "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
        "from pathlib import Path\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "import gzip\n",
        "import html\n",
        "import os\n",
        "from functools import lru_cache\n",
        "\n",
        "import ftfy\n",
        "import regex as re\n",
        "!pip install kornia\n",
        "from kornia.filters import gaussian_blur2d"
      ],
      "metadata": {
        "id": "iX8Sr6-4n46c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f1241bf-853a-4dd3-ccba-29af29c3f120"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kornia in /usr/local/lib/python3.7/dist-packages (0.6.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from kornia) (21.3)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from kornia) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->kornia) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->kornia) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/drive/MyDrive/CS19-1/Personal/Ge/data/hateful_memes/img\"\n",
        "train_path = \"/content/drive/MyDrive/CS19-1/Personal/Ge/data/hateful_memes/annotation/train.jsonl\"\n",
        "dev_path_seen   = \"/content/drive/MyDrive/CS19-1/Personal/Ge/data/hateful_memes/annotation/dev_seen.jsonl\"\n",
        "dev_path_unseen = \"/content/drive/MyDrive/CS19-1/Personal/Ge/data/hateful_memes/annotation/dev_unseen.jsonl\"\n",
        "test_path_seen  = \"/content/drive/MyDrive/CS19-1/Personal/Ge/data/hateful_memes/annotation/test_seen.jsonl\"\n",
        "test_path_unseen  = \"/content/drive/MyDrive/CS19-1/Personal/Ge/data/hateful_memes/annotation/test_unseen.jsonl\""
      ],
      "metadata": {
        "id": "xwEQWBCLrNLk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))"
      ],
      "metadata": {
        "id": "9MXXjmG6MyXq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def basic_clean(text):\n",
        "    text = ftfy.fix_text(text)\n",
        "    text = html.unescape(html.unescape(text))\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def whitespace_clean(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "class SimpleTokenizer(object):\n",
        "    def __init__(self, bpe_path: str = \"bpe_simple_vocab_16e6.txt.gz\"):\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
        "        merges = merges[1:49152-256-2+1]\n",
        "        merges = [tuple(merge.split()) for merge in merges]\n",
        "        vocab = list(bytes_to_unicode().values())\n",
        "        vocab = vocab + [v+'</w>' for v in vocab]\n",
        "        for merge in merges:\n",
        "            vocab.append(''.join(merge))\n",
        "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
        "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
        "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token+'</w>'\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                    new_word.append(first+second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = ' '.join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        text = whitespace_clean(basic_clean(text)).lower()\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = ''.join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
        "        return text"
      ],
      "metadata": {
        "id": "OlLcdtmsM1l0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os, sys\n",
        "# sys.path.append('path_to_the_module/early-stopping-pytorch')\n",
        "\n",
        "# 克服 EarlyStopping 问题 - https://github.com/Bjarten/early-stopping-pytorch - 下载py文件到本地\n",
        "# from pytorchtools import EarlyStopping\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "metadata": {
        "id": "tIDcTEhDN9aC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model = torch.jit.load(\"clip_model.pt\").cuda().eval()\n",
        "input_resolution = clip_model.input_resolution.item()\n",
        "context_length = clip_model.context_length.item()\n",
        "vocab_size = clip_model.vocab_size.item()"
      ],
      "metadata": {
        "id": "BlYDBL_8rgAd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = Compose([\n",
        "    Resize(input_resolution, interpolation=Image.BICUBIC),\n",
        "    CenterCrop(input_resolution),\n",
        "    ToTensor()\n",
        "    ])\n",
        "tokenizer = SimpleTokenizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcfLE6V6rhmS",
        "outputId": "2fce9269-c9be-40e1-ed05-086d04f8c533"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
            "  \"Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the image features for a single image input\n",
        "def process_image_clip(in_img):\n",
        "    image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
        "    image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()\n",
        "    \n",
        "    image = preprocess(Image.open(in_img).convert(\"RGB\"))\n",
        "    \n",
        "    image_input = torch.tensor(np.stack(image)).cuda()\n",
        "    image_input -= image_mean[:, None, None]\n",
        "    image_input /= image_std[:, None, None]\n",
        "    return image_input\n",
        "\n",
        "# Get the text features for a single text input\n",
        "def process_text_clip(in_text):    \n",
        "    text_token = tokenizer.encode(in_text)\n",
        "    text_input = torch.zeros(clip_model.context_length, dtype=torch.long)\n",
        "    sot_token = tokenizer.encoder['<|startoftext|>']\n",
        "    eot_token = tokenizer.encoder['<|endoftext|>']\n",
        "    tokens = [sot_token] + text_token[:75] + [eot_token]\n",
        "    text_input[:len(tokens)] = torch.tensor(tokens)\n",
        "    text_input = text_input.cuda()\n",
        "    return text_input"
      ],
      "metadata": {
        "id": "pNaaZaUyrj1A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HarmemeMemesDatasetAug2(torch.utils.data.Dataset):\n",
        "    \"\"\"Uses jsonl data to preprocess and serve \n",
        "    dictionary of multimodal tensors for model input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        samples_frame,\n",
        "        img_dir,\n",
        "        balance=False,\n",
        "        dev_limit=None,\n",
        "        random_state=0\n",
        "        ):\n",
        "      \n",
        "        self.samples_frame = samples_frame\n",
        "\n",
        "        self.samples_frame = self.samples_frame.reset_index(\n",
        "            drop=True\n",
        "        )\n",
        "        self.samples_frame.image = self.samples_frame.apply(\n",
        "            lambda row: (img_dir + '/' + row.img), axis=1\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "    def __len__(self):\n",
        "        \"\"\"This method is called when you do len(instance) \n",
        "        for an instance of this class.\n",
        "        \"\"\"\n",
        "        return len(self.samples_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"This method is called when you do instance[key] \n",
        "        for an instance of this class.\n",
        "        \"\"\"\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_id = self.samples_frame.loc[idx, \"id\"]\n",
        "        img_file_name = self.samples_frame.loc[idx, \"img\"]\n",
        "        \n",
        "        image_clip_input = process_image_clip(\"/content/drive/MyDrive/CS19-1/Personal/Ge/data/hateful_memes/\"+img_file_name)\n",
        "\n",
        "        text_clip_input = process_text_clip(self.samples_frame.loc[idx, \"text\"])\n",
        "\n",
        "        if \"label\" in self.samples_frame.columns:\n",
        "            lab = self.samples_frame.loc[idx, \"label\"]\n",
        "            label = torch.tensor(lab).to(device)  \n",
        "            \n",
        "            sample = {\n",
        "                \"id\": img_id, \n",
        "                \"image_clip_input\": image_clip_input,\n",
        "                \"text_clip_input\": text_clip_input,\n",
        "                \"label\": label\n",
        "            }\n",
        "        else:\n",
        "            sample = {\n",
        "                \"id\": img_id, \n",
        "                \"image_clip_input\": image_clip_input,\n",
        "                \"text_clip_input\": text_clip_input\n",
        "            }\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "R5lz3mmHNdXV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples_frame = pd.read_json(train_path, lines=True)\n",
        "print(train_samples_frame.shape)\n",
        "\n",
        "dev_samples_frame_unseen = pd.read_json(dev_path_unseen, lines=True)\n",
        "print(dev_samples_frame_unseen.shape)\n",
        "\n",
        "test_samples_frame_unseen = pd.read_json(test_path_unseen, lines=True)\n",
        "print(test_samples_frame_unseen.shape)\n",
        "\n",
        "test_samples_frame_seen = pd.read_json(test_path_seen, lines=True)\n",
        "print(test_samples_frame_seen.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT_Q7QNBuvyw",
        "outputId": "08df1667-ad7e-4036-b884-9d6e3bafce96"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8500, 4)\n",
            "(540, 4)\n",
            "(2000, 4)\n",
            "(1000, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hm_dataset_train = HarmemeMemesDatasetAug2(train_samples_frame, data_dir)\n",
        "dataloader_train = DataLoader(hm_dataset_train, batch_size=64,\n",
        "                        shuffle=True, num_workers=0)\n",
        "hm_dataset_val = HarmemeMemesDatasetAug2(dev_samples_frame_unseen, data_dir)\n",
        "dataloader_val = DataLoader(hm_dataset_val, batch_size=64,\n",
        "                        shuffle=True, num_workers=0)\n",
        "hm_dataset_test = HarmemeMemesDatasetAug2(test_samples_frame_seen, data_dir)\n",
        "dataloader_test = DataLoader(hm_dataset_test, batch_size=64,\n",
        "                             shuffle=False, num_workers=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "VcKqmn8JKaDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2efe602c-0b59-44ff-d9a7-53dfe52810bc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SinkhornSolver(nn.Module):\n",
        "    \"\"\"\n",
        "    Optimal Transport solver under entropic regularisation.\n",
        "    Based on the code of Gabriel Peyré.\n",
        "    \"\"\"\n",
        "    def __init__(self, epsilon, iterations=100, ground_metric=lambda x: torch.pow(x, 2)):\n",
        "        super(SinkhornSolver, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.iterations = iterations\n",
        "        self.ground_metric = ground_metric\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        num_x = x.size(-2)\n",
        "        num_y = y.size(-2)\n",
        "        \n",
        "        batch_size = 1 if x.dim() == 2 else x.size(0)\n",
        "\n",
        "        # Marginal densities are empirical measures\n",
        "        a = x.new_ones((batch_size, num_x), requires_grad=False) / num_x\n",
        "        b = y.new_ones((batch_size, num_y), requires_grad=False) / num_y\n",
        "        \n",
        "        a = a.squeeze()\n",
        "        b = b.squeeze()\n",
        "                \n",
        "        # Initialise approximation vectors in log domain\n",
        "        u = torch.zeros_like(a)\n",
        "        v = torch.zeros_like(b)\n",
        "\n",
        "        # Stopping criterion\n",
        "        threshold = 1e-1\n",
        "        \n",
        "        # Cost matrix\n",
        "        C = self._compute_cost(x, y)\n",
        "        \n",
        "        # Sinkhorn iterations\n",
        "        for i in range(self.iterations): \n",
        "            u0, v0 = u, v\n",
        "                        \n",
        "            # u^{l+1} = a / (K v^l)\n",
        "            K = self._log_boltzmann_kernel(u, v, C)\n",
        "            u_ = torch.log(a + 1e-8) - torch.logsumexp(K, dim=1)\n",
        "            u = self.epsilon * u_ + u\n",
        "                        \n",
        "            # v^{l+1} = b / (K^T u^(l+1))\n",
        "            K_t = self._log_boltzmann_kernel(u, v, C).transpose(-2, -1)\n",
        "            v_ = torch.log(b + 1e-8) - torch.logsumexp(K_t, dim=1)\n",
        "            v = self.epsilon * v_ + v\n",
        "            \n",
        "            # Size of the change we have performed on u\n",
        "            diff = torch.sum(torch.abs(u - u0), dim=-1) + torch.sum(torch.abs(v - v0), dim=-1)\n",
        "            mean_diff = torch.mean(diff)\n",
        "                        \n",
        "            if mean_diff.item() < threshold:\n",
        "                break\n",
        "   \n",
        "        # print(\"Finished computing transport plan in {} iterations\".format(i))\n",
        "    \n",
        "        # Transport plan pi = diag(a)*K*diag(b)\n",
        "        K = self._log_boltzmann_kernel(u, v, C)\n",
        "        pi = torch.exp(K)\n",
        "        \n",
        "        # Sinkhorn distance\n",
        "        cost = torch.sum(pi * C, dim=(-2, -1))\n",
        "\n",
        "        return cost, pi\n",
        "\n",
        "    def _compute_cost(self, x, y):\n",
        "        x_ = x.unsqueeze(-2)\n",
        "        y_ = y.unsqueeze(-3)\n",
        "        C = torch.sum(self.ground_metric(x_ - y_), dim=-1)\n",
        "        return C\n",
        "\n",
        "    def _log_boltzmann_kernel(self, u, v, C=None):\n",
        "        C = self._compute_cost(x, y) if C is None else C\n",
        "        kernel = -C + u.unsqueeze(-1) + v.unsqueeze(-2)\n",
        "        kernel /= self.epsilon\n",
        "        return kernel\n",
        "\n",
        "def sinkhorn_iter(x,y):\n",
        "    epsilon = 10**(-(2))\n",
        "    solver = SinkhornSolver(epsilon=epsilon, iterations=10000)\n",
        "    cost, pi = solver.forward(x, y)\n",
        "    return cost,pi"
      ],
      "metadata": {
        "id": "d3jYr15Eg1D7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the model"
      ],
      "metadata": {
        "id": "CGZXIsXrMsmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "5ZL13U_2oGwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aa = nn.Linear(64, 2).to(device)\n",
        "for data in dataloader_train_pol:\n",
        "    a = data[\"image_clip_input\"]\n",
        "    b = data[\"text_clip_input\"]\n",
        "    img_feat_clip = clip_model.encode_image(a).float().to(device)\n",
        "    txt_feat_clip = clip_model.encode_text(b).float().to(device)\n",
        "    in_CI_smo = gaussian_blur2d(img_feat_clip.unsqueeze(0).unsqueeze(0),(3, 3), (1.5, 1.5)) #[1, 1, 64, 512]\n",
        "    in_CT_smo = gaussian_blur2d(txt_feat_clip.unsqueeze(0).unsqueeze(0),(3, 3), (1.5, 1.5)) #[1, 1, 64, 512]\n",
        "    cost,pi = sinkhorn_iter(in_CI_smo.squeeze(0).squeeze(0),in_CT_smo.squeeze(0).squeeze(0))\n",
        "    print(F.relu(aa(pi)))\n",
        "    break"
      ],
      "metadata": {
        "id": "kwymCSemPgvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = nn.Linear(64, 2).to(device)\n",
        "F.sigmoid(a(pi))"
      ],
      "metadata": {
        "id": "s65qhNmcPdi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the cross attention value features \n",
        "# Vanilla model\n",
        "\n",
        "class MM(nn.Module):\n",
        "    def __init__(self, n_out):\n",
        "        super(MM, self).__init__()  \n",
        "        self.dense_vgg_1024 = nn.Linear(4096, 1024)\n",
        "        self.dense_vgg_512 = nn.Linear(1024, 512)\n",
        "        self.drop20 = nn.Dropout(p=0.2)\n",
        "        self.drop5 = nn.Dropout(p=0.05) \n",
        "        \n",
        "        self.dense_drob_512 = nn.Linear(768, 512)\n",
        "        \n",
        "        self.gen_key_L1 = nn.Linear(512, 256) # 512X256\n",
        "        self.gen_query_L1 = nn.Linear(512, 256) # 512X256\n",
        "        self.gen_key_L2 = nn.Linear(512, 256) # 512X256\n",
        "        self.gen_query_L2 = nn.Linear(512, 256) # 512X256\n",
        "        self.gen_key_L3 = nn.Linear(512, 256) # 512X256\n",
        "        self.gen_query_L3 = nn.Linear(512, 256) # 512X256\n",
        "#         self.gen_value = nn.Linear(512, 256) # 512X256\n",
        "        self.soft = nn.Softmax(dim=1)\n",
        "        self.soft_final = nn.Softmax(dim=1)\n",
        "        self.project_dense_512a = nn.Linear(1024, 512) # 512X256\n",
        "        self.project_dense_512b = nn.Linear(1024, 512) # 512X256\n",
        "        self.project_dense_512c = nn.Linear(1024, 512) # 512X256 \n",
        "        \n",
        "        \n",
        "        self.fc_out = nn.Linear(512, 256) # 512X256\n",
        "        self.sink_out = nn.Linear(64, 2) # 512X256\n",
        "        self.out = nn.Linear(256, n_out) # 512X256\n",
        "        \n",
        "    def selfattNFuse(self, vec1, vec2): \n",
        "            q1 = F.relu(self.gen_query_L1(vec1))\n",
        "            k1 = F.relu(self.gen_key_L1(vec1))\n",
        "            q2 = F.relu(self.gen_query_L1(vec2))\n",
        "            k2 = F.relu(self.gen_key_L1(vec2))\n",
        "            score1 = torch.reshape(torch.bmm(q1.view(-1, 1, 256), k2.view(-1, 256, 1)), (-1, 1))\n",
        "            score2 = torch.reshape(torch.bmm(q2.view(-1, 1, 256), k1.view(-1, 256, 1)), (-1, 1))\n",
        "            wt_score1_score2_mat = torch.cat((score1, score2), 1)\n",
        "            wt_i1_i2 = self.soft(wt_score1_score2_mat.float()) #prob\n",
        "            prob_1 = wt_i1_i2[:,0]\n",
        "            prob_2 = wt_i1_i2[:,1]\n",
        "            wtd_i1 = vec1 * prob_1[:, None]\n",
        "            wtd_i2 = vec2 * prob_2[:, None]\n",
        "            out_rep = F.relu(self.project_dense_512a(torch.cat((wtd_i1,wtd_i2), 1)))\n",
        "            return out_rep\n",
        "  \n",
        "\n",
        "\n",
        "    def forward(self, in_CI, in_CT):\n",
        "        # print(in_CI.shape,in_CT.shape)\n",
        "        in_CI_smo = gaussian_blur2d(in_CI.unsqueeze(0).unsqueeze(0),(3, 3), (1.5, 1.5)) #[1, 1, 64, 512]\n",
        "        in_CT_smo = gaussian_blur2d(in_CT.unsqueeze(0).unsqueeze(0),(3, 3), (1.5, 1.5)) #[1, 1, 64, 512]\n",
        "\n",
        "        cost,pi = sinkhorn_iter(in_CI_smo.squeeze(0).squeeze(0),in_CT_smo.squeeze(0).squeeze(0)) #64x64\n",
        "        score_sink = F.sigmoid(self.sink_out(pi)) #64x2\n",
        "        # print(score_sink)\n",
        "        res_inter = self.selfattNFuse(in_CI, in_CT) #64x512\n",
        "        res_inter_score = F.relu(self.fc_out(res_inter)) #64x256\n",
        "        res_inter_score_final = torch.sigmoid(self.out(res_inter_score)) #64x2\n",
        "        score_sum = res_inter_score_final + score_sink\n",
        "        return score_sum"
      ],
      "metadata": {
        "id": "9wlXGmNlN20i"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training function\n"
      ],
      "metadata": {
        "id": "xJMN8x1BMv0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For cross entropy loss\n",
        "def train_model(model, patience, n_epochs, dataloader_train, dataloader_val):\n",
        "    epochs = n_epochs\n",
        "#     clip = 5\n",
        "\n",
        "    train_acc_list=[]\n",
        "    val_acc_list=[]\n",
        "    train_loss_list=[]\n",
        "    val_loss_list=[]\n",
        "    \n",
        "        # initialize the experiment path\n",
        "    Path(exp_path).mkdir(parents=True, exist_ok=True)\n",
        "    # initialize early_stopping object\n",
        "    chk_file = os.path.join(exp_path, 'checkpoint_'+exp_name+'.pt')\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True, path=chk_file)\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    for i in range(epochs):\n",
        "#         total_acc_train = 0\n",
        "        total_loss_train = 0\n",
        "        total_train = 0\n",
        "        correct_train = 0\n",
        "\n",
        "        for ii,data in enumerate(dataloader_train):\n",
        "            \n",
        "#             Clip features...\n",
        "            img_inp_clip = data['image_clip_input']\n",
        "            txt_inp_clip = data['text_clip_input']\n",
        "            # bert_in = data['text_drob_feature']\n",
        "            with torch.no_grad():\n",
        "                img_feat_clip = clip_model.encode_image(img_inp_clip).float().to(device)\n",
        "                txt_feat_clip = clip_model.encode_text(txt_inp_clip).float().to(device)\n",
        "                # global_img = global_feature[0].float().to(device)\n",
        "                # global_txt = global_feature[1].float().to(device)\n",
        "\n",
        "            label_train = data['label'].to(device)\n",
        "\n",
        "            model.zero_grad()\n",
        "            try:\n",
        "                output = model(img_feat_clip, txt_feat_clip)\n",
        "                # print(output.squeeze())\n",
        "                loss = criterion(output.squeeze(), label_train)\n",
        "                \n",
        "                loss.backward()\n",
        "    #             nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "                optimizer.step()\n",
        "            \n",
        "\n",
        "                with torch.no_grad():\n",
        "                    _, predicted_train = torch.max(output.data, 1)\n",
        "                    total_train += label_train.size(0)\n",
        "                    correct_train += (predicted_train == label_train).sum().item()\n",
        "    #                 out_val = (output.squeeze()>0.5).float()\n",
        "    #                 out_final = ((out_val == 1).nonzero(as_tuple=True)[0])\n",
        "    #                 print()\n",
        "    #                 acc = torch.abs(output.squeeze() - label.float()).view(-1)\n",
        "    #                 acc = (1. - acc.sum() / acc.size()[0])\n",
        "    #                 total_acc_train += acc\n",
        "                    total_loss_train += loss.item()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            if ii%5 == 0:\n",
        "                print(f\"epoch:{i}, batch:{ii}\")\n",
        "\n",
        "        # print(label_train,correct_train,total_train)\n",
        "        train_acc = 100 * correct_train / total_train\n",
        "        train_loss = total_loss_train/total_train\n",
        "        model.eval()\n",
        "#         total_acc_val = 0\n",
        "        total_loss_val = 0\n",
        "        total_val = 0\n",
        "        correct_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in dataloader_val:                \n",
        "#                 Clip features...                \n",
        "                img_inp_clip = data['image_clip_input']\n",
        "                txt_inp_clip = data['text_clip_input']\n",
        "                # bert_in = data['text_drob_feature']\n",
        "                with torch.no_grad():\n",
        "                    img_feat_clip = clip_model.encode_image(img_inp_clip).float().to(device)\n",
        "                    txt_feat_clip = clip_model.encode_text(txt_inp_clip).float().to(device)\n",
        "                    # global_img = global_feature[0].float().to(device)\n",
        "                    # global_txt = global_feature[1].float().to(device)\n",
        "        \n",
        "                label_val = data['label'].to(device)\n",
        "\n",
        "                model.zero_grad()\n",
        "                try:\n",
        "                    output = model(img_feat_clip, txt_feat_clip)\n",
        "                    \n",
        "                    \n",
        "                    val_loss = criterion(output.squeeze(), label_val)\n",
        "                    _, predicted_val = torch.max(output.data, 1)\n",
        "                    total_val += label_val.size(0)\n",
        "                    correct_val += (predicted_val == label_val).sum().item()                \n",
        "                    total_loss_val += val_loss.item()\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        if i == 10:\n",
        "            print(\"Saving model...\") \n",
        "            torch.save(model.state_dict(), os.path.join(exp_path, \"10epoch_model.pt\"))\n",
        "        val_acc = 100 * correct_val / total_val\n",
        "        val_loss = total_loss_val/total_val\n",
        "\n",
        "        train_acc_list.append(train_acc)\n",
        "        val_acc_list.append(val_acc)\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_loss_list.append(val_loss)\n",
        "        \n",
        "        early_stopping(val_loss, model)\n",
        "        \n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "            \n",
        "\n",
        "        print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
        "        model.train()\n",
        "        torch.cuda.empty_cache()\n",
        "    print(\"Saving model...\") \n",
        "    torch.save(model.state_dict(), os.path.join(exp_path, \"final_model.pt\"))\n",
        "    # load the last checkpoint with the best model\n",
        "#     model.load_state_dict(torch.load('checkpoint_1.pt'))\n",
        "    \n",
        "    return  model, train_acc_list, val_acc_list, train_loss_list, val_loss_list, i"
      ],
      "metadata": {
        "id": "oVB4mHJ0OBPF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For CE loss\n",
        "def test_model(model, dataloader_test,test_frame):\n",
        "    ls_truth = []\n",
        "    model.eval()\n",
        "    total_test = 0\n",
        "    correct_test =0\n",
        "    total_acc_test = 0\n",
        "    total_loss_test = 0\n",
        "    outputs = []\n",
        "    test_labels=[]\n",
        "    with torch.no_grad():\n",
        "        for i,data in enumerate(dataloader_test):\n",
        "            img_inp_clip = data['image_clip_input']\n",
        "            txt_inp_clip = data['text_clip_input']\n",
        "            # bert_in = data['text_drob_feature']\n",
        "            with torch.no_grad():\n",
        "                img_feat_clip = clip_model.encode_image(img_inp_clip).float().to(device)\n",
        "                txt_feat_clip = clip_model.encode_text(txt_inp_clip).float().to(device)\n",
        "                # global_feature = clip_model(img_inp_clip,txt_inp_clip)\n",
        "                # global_img = global_feature[0].float().to(device)\n",
        "                # global_txt = global_feature[1].float().to(device)          \n",
        "\n",
        "            label_test = data['label'].to(device)\n",
        "            \n",
        "#             out = model(img_feat_vgg, txt_feat_trans)        \n",
        "            try:\n",
        "                out = model(img_feat_clip, txt_feat_clip)        \n",
        "\n",
        "                outputs += list(out.cpu().data.numpy())\n",
        "                loss = criterion(out.squeeze(), label_test)\n",
        "                \n",
        "                _, predicted_test = torch.max(out.data, 1)\n",
        "                total_test += label_test.size(0)\n",
        "                correct_test += (predicted_test == label_test).sum().item()\n",
        "    #                 out_val = (output.squeeze()>0.5).float()\n",
        "    #                 out_final = ((out_val == 1).nonzero(as_tuple=True)[0])\n",
        "    #                 print()\n",
        "    #                 acc = torch.abs(output.squeeze() - label.float()).view(-1)\n",
        "    #                 acc = (1. - acc.sum() / acc.size()[0])\n",
        "    #                 total_acc_train += acc\n",
        "                ls_truth.append(label_test)\n",
        "            except:\n",
        "                pass\n",
        "            \n",
        "            \n",
        "#     #         print(label.float())\n",
        "#             acc = torch.abs(out.squeeze() - label.float()).view(-1)\n",
        "#     #         print((acc.sum() / acc.size()[0]))\n",
        "#             acc = (1. - acc.sum() / acc.size()[0])\n",
        "#     #         print(acc)\n",
        "#             total_acc_test += acc\n",
        "#             total_loss_test += loss.item()\n",
        "\n",
        "    \n",
        "    acc_test = 100 * correct_test / total_test\n",
        "    loss_test = total_loss_test/total_test   \n",
        "\n",
        "    return outputs,ls_truth\n"
      ],
      "metadata": {
        "id": "AsplvOxzOIkN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "MIcLmqDwN3_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_size = 2\n",
        "exp_name = \"EMNLP_MCHarm_GLAREAll_POLTrain_POLEval\"\n",
        "# pre_trn_ckp = \"EMNLP_MCHarm_GLAREAll_COVTrain\" # Uncomment for using pre-trained\n",
        "exp_path = \"path_to_saved_files/EMNLP_ModelCkpt/\"+exp_name\n",
        "lr=0.0007\n",
        "# criterion = nn.BCELoss() #Binary case\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# # ------------Fresh training------------\n",
        "model = MM(output_size)\n",
        "model.to(device)\n",
        "# print(model_harm_p)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "n_epochs = 15\n",
        "# early stopping patience; how long to wait after last time validation loss improved.\n",
        "patience = 25\n",
        "model_harm_c, train_acc_list_cov, val_acc_list_cov, train_loss_list_cov, val_loss_list_cov, epoc_num = train_model(model, patience,\n",
        "                                                                                            n_epochs,dataloader_train,\n",
        "                                                                                            dataloader_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGuZOSQOKO3U",
        "outputId": "0b0951c6-25ec-4532-9e96-ff2222443104"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:594.)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0, batch:0\n",
            "epoch:0, batch:5\n",
            "epoch:0, batch:10\n",
            "epoch:0, batch:15\n",
            "epoch:0, batch:20\n",
            "epoch:0, batch:25\n",
            "epoch:0, batch:30\n",
            "epoch:0, batch:35\n",
            "epoch:0, batch:40\n",
            "epoch:0, batch:45\n",
            "epoch:0, batch:50\n",
            "epoch:0, batch:55\n",
            "epoch:0, batch:60\n",
            "epoch:0, batch:65\n",
            "epoch:0, batch:70\n",
            "epoch:0, batch:75\n",
            "epoch:0, batch:80\n",
            "epoch:0, batch:85\n",
            "epoch:0, batch:90\n",
            "epoch:0, batch:95\n",
            "epoch:0, batch:100\n",
            "epoch:0, batch:105\n",
            "epoch:0, batch:110\n",
            "epoch:0, batch:115\n",
            "epoch:0, batch:120\n",
            "epoch:0, batch:125\n",
            "epoch:0, batch:130\n",
            "Validation loss decreased (inf --> 0.010289).  Saving model ...\n",
            "Epoch 1: train_loss: 0.0092 train_acc: 70.6558 | val_loss: 0.0103 val_acc: 63.4766\n",
            "epoch:1, batch:0\n",
            "epoch:1, batch:5\n",
            "epoch:1, batch:10\n",
            "epoch:1, batch:15\n",
            "epoch:1, batch:20\n",
            "epoch:1, batch:25\n",
            "epoch:1, batch:30\n",
            "epoch:1, batch:35\n",
            "epoch:1, batch:40\n",
            "epoch:1, batch:45\n",
            "epoch:1, batch:50\n",
            "epoch:1, batch:55\n",
            "epoch:1, batch:60\n",
            "epoch:1, batch:65\n",
            "epoch:1, batch:70\n",
            "epoch:1, batch:75\n",
            "epoch:1, batch:80\n",
            "epoch:1, batch:85\n",
            "epoch:1, batch:90\n",
            "epoch:1, batch:95\n",
            "epoch:1, batch:100\n",
            "epoch:1, batch:105\n",
            "epoch:1, batch:110\n",
            "epoch:1, batch:115\n",
            "epoch:1, batch:120\n",
            "epoch:1, batch:125\n",
            "epoch:1, batch:130\n",
            "Validation loss decreased (0.010289 --> 0.009833).  Saving model ...\n",
            "Epoch 2: train_loss: 0.0083 train_acc: 77.2964 | val_loss: 0.0098 val_acc: 66.9922\n",
            "epoch:2, batch:0\n",
            "epoch:2, batch:5\n",
            "epoch:2, batch:10\n",
            "epoch:2, batch:15\n",
            "epoch:2, batch:20\n",
            "epoch:2, batch:25\n",
            "epoch:2, batch:30\n",
            "epoch:2, batch:35\n",
            "epoch:2, batch:40\n",
            "epoch:2, batch:45\n",
            "epoch:2, batch:50\n",
            "epoch:2, batch:55\n",
            "epoch:2, batch:60\n",
            "epoch:2, batch:65\n",
            "epoch:2, batch:70\n",
            "epoch:2, batch:75\n",
            "epoch:2, batch:80\n",
            "epoch:2, batch:85\n",
            "epoch:2, batch:90\n",
            "epoch:2, batch:95\n",
            "epoch:2, batch:100\n",
            "epoch:2, batch:105\n",
            "epoch:2, batch:110\n",
            "epoch:2, batch:115\n",
            "epoch:2, batch:120\n",
            "epoch:2, batch:125\n",
            "epoch:2, batch:130\n",
            "EarlyStopping counter: 1 out of 25\n",
            "Epoch 3: train_loss: 0.0078 train_acc: 80.3030 | val_loss: 0.0100 val_acc: 65.8203\n",
            "epoch:3, batch:0\n",
            "epoch:3, batch:5\n",
            "epoch:3, batch:10\n",
            "epoch:3, batch:15\n",
            "epoch:3, batch:20\n",
            "epoch:3, batch:25\n",
            "epoch:3, batch:30\n",
            "epoch:3, batch:35\n",
            "epoch:3, batch:40\n",
            "epoch:3, batch:45\n",
            "epoch:3, batch:50\n",
            "epoch:3, batch:55\n",
            "epoch:3, batch:60\n",
            "epoch:3, batch:65\n",
            "epoch:3, batch:70\n",
            "epoch:3, batch:75\n",
            "epoch:3, batch:80\n",
            "epoch:3, batch:85\n",
            "epoch:3, batch:90\n",
            "epoch:3, batch:95\n",
            "epoch:3, batch:100\n",
            "epoch:3, batch:105\n",
            "epoch:3, batch:110\n",
            "epoch:3, batch:115\n",
            "epoch:3, batch:120\n",
            "epoch:3, batch:125\n",
            "epoch:3, batch:130\n",
            "EarlyStopping counter: 2 out of 25\n",
            "Epoch 4: train_loss: 0.0074 train_acc: 83.0966 | val_loss: 0.0101 val_acc: 66.2109\n",
            "epoch:4, batch:0\n",
            "epoch:4, batch:5\n",
            "epoch:4, batch:10\n",
            "epoch:4, batch:15\n",
            "epoch:4, batch:20\n",
            "epoch:4, batch:25\n",
            "epoch:4, batch:30\n",
            "epoch:4, batch:35\n",
            "epoch:4, batch:40\n",
            "epoch:4, batch:45\n",
            "epoch:4, batch:50\n",
            "epoch:4, batch:55\n",
            "epoch:4, batch:60\n",
            "epoch:4, batch:65\n",
            "epoch:4, batch:70\n",
            "epoch:4, batch:75\n",
            "epoch:4, batch:80\n",
            "epoch:4, batch:85\n",
            "epoch:4, batch:90\n",
            "epoch:4, batch:95\n",
            "epoch:4, batch:100\n",
            "epoch:4, batch:105\n",
            "epoch:4, batch:110\n",
            "epoch:4, batch:115\n",
            "epoch:4, batch:120\n",
            "epoch:4, batch:125\n",
            "epoch:4, batch:130\n",
            "EarlyStopping counter: 3 out of 25\n",
            "Epoch 5: train_loss: 0.0071 train_acc: 85.0971 | val_loss: 0.0100 val_acc: 66.6016\n",
            "epoch:5, batch:0\n",
            "epoch:5, batch:5\n",
            "epoch:5, batch:10\n",
            "epoch:5, batch:15\n",
            "epoch:5, batch:20\n",
            "epoch:5, batch:25\n",
            "epoch:5, batch:30\n",
            "epoch:5, batch:35\n",
            "epoch:5, batch:40\n",
            "epoch:5, batch:45\n",
            "epoch:5, batch:50\n",
            "epoch:5, batch:55\n",
            "epoch:5, batch:60\n",
            "epoch:5, batch:65\n",
            "epoch:5, batch:70\n",
            "epoch:5, batch:75\n",
            "epoch:5, batch:80\n",
            "epoch:5, batch:85\n",
            "epoch:5, batch:90\n",
            "epoch:5, batch:95\n",
            "epoch:5, batch:100\n",
            "epoch:5, batch:105\n",
            "epoch:5, batch:110\n",
            "epoch:5, batch:115\n",
            "epoch:5, batch:120\n",
            "epoch:5, batch:125\n",
            "epoch:5, batch:130\n",
            "EarlyStopping counter: 4 out of 25\n",
            "Epoch 6: train_loss: 0.0069 train_acc: 86.5885 | val_loss: 0.0106 val_acc: 61.5234\n",
            "epoch:6, batch:0\n",
            "epoch:6, batch:5\n",
            "epoch:6, batch:10\n",
            "epoch:6, batch:15\n",
            "epoch:6, batch:20\n",
            "epoch:6, batch:25\n",
            "epoch:6, batch:30\n",
            "epoch:6, batch:35\n",
            "epoch:6, batch:40\n",
            "epoch:6, batch:45\n",
            "epoch:6, batch:50\n",
            "epoch:6, batch:55\n",
            "epoch:6, batch:60\n",
            "epoch:6, batch:65\n",
            "epoch:6, batch:70\n",
            "epoch:6, batch:75\n",
            "epoch:6, batch:80\n",
            "epoch:6, batch:85\n",
            "epoch:6, batch:90\n",
            "epoch:6, batch:95\n",
            "epoch:6, batch:100\n",
            "epoch:6, batch:105\n",
            "epoch:6, batch:110\n",
            "epoch:6, batch:115\n",
            "epoch:6, batch:120\n",
            "epoch:6, batch:125\n",
            "epoch:6, batch:130\n",
            "EarlyStopping counter: 5 out of 25\n",
            "Epoch 7: train_loss: 0.0067 train_acc: 87.6894 | val_loss: 0.0099 val_acc: 67.1875\n",
            "epoch:7, batch:0\n",
            "epoch:7, batch:5\n",
            "epoch:7, batch:10\n",
            "epoch:7, batch:15\n",
            "epoch:7, batch:20\n",
            "epoch:7, batch:25\n",
            "epoch:7, batch:30\n",
            "epoch:7, batch:35\n",
            "epoch:7, batch:40\n",
            "epoch:7, batch:45\n",
            "epoch:7, batch:50\n",
            "epoch:7, batch:55\n",
            "epoch:7, batch:60\n",
            "epoch:7, batch:65\n",
            "epoch:7, batch:70\n",
            "epoch:7, batch:75\n",
            "epoch:7, batch:80\n",
            "epoch:7, batch:85\n",
            "epoch:7, batch:90\n",
            "epoch:7, batch:95\n",
            "epoch:7, batch:100\n",
            "epoch:7, batch:105\n",
            "epoch:7, batch:110\n",
            "epoch:7, batch:115\n",
            "epoch:7, batch:120\n",
            "epoch:7, batch:125\n",
            "epoch:7, batch:130\n",
            "EarlyStopping counter: 6 out of 25\n",
            "Epoch 8: train_loss: 0.0066 train_acc: 87.9380 | val_loss: 0.0099 val_acc: 66.9922\n",
            "epoch:8, batch:0\n",
            "epoch:8, batch:5\n",
            "epoch:8, batch:10\n",
            "epoch:8, batch:15\n",
            "epoch:8, batch:20\n",
            "epoch:8, batch:25\n",
            "epoch:8, batch:30\n",
            "epoch:8, batch:35\n",
            "epoch:8, batch:40\n",
            "epoch:8, batch:45\n",
            "epoch:8, batch:50\n",
            "epoch:8, batch:55\n",
            "epoch:8, batch:60\n",
            "epoch:8, batch:65\n",
            "epoch:8, batch:70\n",
            "epoch:8, batch:75\n",
            "epoch:8, batch:80\n",
            "epoch:8, batch:85\n",
            "epoch:8, batch:90\n",
            "epoch:8, batch:95\n",
            "epoch:8, batch:100\n",
            "epoch:8, batch:105\n",
            "epoch:8, batch:110\n",
            "epoch:8, batch:115\n",
            "epoch:8, batch:120\n",
            "epoch:8, batch:125\n",
            "epoch:8, batch:130\n",
            "EarlyStopping counter: 7 out of 25\n",
            "Epoch 9: train_loss: 0.0064 train_acc: 89.5597 | val_loss: 0.0100 val_acc: 66.4062\n",
            "epoch:9, batch:0\n",
            "epoch:9, batch:5\n",
            "epoch:9, batch:10\n",
            "epoch:9, batch:15\n",
            "epoch:9, batch:20\n",
            "epoch:9, batch:25\n",
            "epoch:9, batch:30\n",
            "epoch:9, batch:35\n",
            "epoch:9, batch:40\n",
            "epoch:9, batch:45\n",
            "epoch:9, batch:50\n",
            "epoch:9, batch:55\n",
            "epoch:9, batch:60\n",
            "epoch:9, batch:65\n",
            "epoch:9, batch:70\n",
            "epoch:9, batch:75\n",
            "epoch:9, batch:80\n",
            "epoch:9, batch:85\n",
            "epoch:9, batch:90\n",
            "epoch:9, batch:95\n",
            "epoch:9, batch:100\n",
            "epoch:9, batch:105\n",
            "epoch:9, batch:110\n",
            "epoch:9, batch:115\n",
            "epoch:9, batch:120\n",
            "epoch:9, batch:125\n",
            "epoch:9, batch:130\n",
            "EarlyStopping counter: 8 out of 25\n",
            "Epoch 10: train_loss: 0.0064 train_acc: 89.6070 | val_loss: 0.0101 val_acc: 65.6250\n",
            "epoch:10, batch:0\n",
            "epoch:10, batch:5\n",
            "epoch:10, batch:10\n",
            "epoch:10, batch:15\n",
            "epoch:10, batch:20\n",
            "epoch:10, batch:25\n",
            "epoch:10, batch:30\n",
            "epoch:10, batch:35\n",
            "epoch:10, batch:40\n",
            "epoch:10, batch:45\n",
            "epoch:10, batch:50\n",
            "epoch:10, batch:55\n",
            "epoch:10, batch:60\n",
            "epoch:10, batch:65\n",
            "epoch:10, batch:70\n",
            "epoch:10, batch:75\n",
            "epoch:10, batch:80\n",
            "epoch:10, batch:85\n",
            "epoch:10, batch:90\n",
            "epoch:10, batch:95\n",
            "epoch:10, batch:100\n",
            "epoch:10, batch:105\n",
            "epoch:10, batch:110\n",
            "epoch:10, batch:115\n",
            "epoch:10, batch:120\n",
            "epoch:10, batch:125\n",
            "epoch:10, batch:130\n",
            "Saving model...\n",
            "EarlyStopping counter: 9 out of 25\n",
            "Epoch 11: train_loss: 0.0062 train_acc: 90.5303 | val_loss: 0.0098 val_acc: 67.3828\n",
            "epoch:11, batch:0\n",
            "epoch:11, batch:5\n",
            "epoch:11, batch:10\n",
            "epoch:11, batch:15\n",
            "epoch:11, batch:20\n",
            "epoch:11, batch:25\n",
            "epoch:11, batch:30\n",
            "epoch:11, batch:35\n",
            "epoch:11, batch:40\n",
            "epoch:11, batch:45\n",
            "epoch:11, batch:50\n",
            "epoch:11, batch:55\n",
            "epoch:11, batch:60\n",
            "epoch:11, batch:65\n",
            "epoch:11, batch:70\n",
            "epoch:11, batch:75\n",
            "epoch:11, batch:80\n",
            "epoch:11, batch:85\n",
            "epoch:11, batch:90\n",
            "epoch:11, batch:95\n",
            "epoch:11, batch:100\n",
            "epoch:11, batch:105\n",
            "epoch:11, batch:110\n",
            "epoch:11, batch:115\n",
            "epoch:11, batch:120\n",
            "epoch:11, batch:125\n",
            "epoch:11, batch:130\n",
            "Validation loss decreased (0.009833 --> 0.009758).  Saving model ...\n",
            "Epoch 12: train_loss: 0.0062 train_acc: 90.3527 | val_loss: 0.0098 val_acc: 67.1875\n",
            "epoch:12, batch:0\n",
            "epoch:12, batch:5\n",
            "epoch:12, batch:10\n",
            "epoch:12, batch:15\n",
            "epoch:12, batch:20\n",
            "epoch:12, batch:25\n",
            "epoch:12, batch:30\n",
            "epoch:12, batch:35\n",
            "epoch:12, batch:40\n",
            "epoch:12, batch:45\n",
            "epoch:12, batch:50\n",
            "epoch:12, batch:55\n",
            "epoch:12, batch:60\n",
            "epoch:12, batch:65\n",
            "epoch:12, batch:70\n",
            "epoch:12, batch:75\n",
            "epoch:12, batch:80\n",
            "epoch:12, batch:85\n",
            "epoch:12, batch:90\n",
            "epoch:12, batch:95\n",
            "epoch:12, batch:100\n",
            "epoch:12, batch:105\n",
            "epoch:12, batch:110\n",
            "epoch:12, batch:115\n",
            "epoch:12, batch:120\n",
            "epoch:12, batch:125\n",
            "epoch:12, batch:130\n",
            "EarlyStopping counter: 1 out of 25\n",
            "Epoch 13: train_loss: 0.0062 train_acc: 90.7197 | val_loss: 0.0100 val_acc: 67.3828\n",
            "epoch:13, batch:0\n",
            "epoch:13, batch:5\n",
            "epoch:13, batch:10\n",
            "epoch:13, batch:15\n",
            "epoch:13, batch:20\n",
            "epoch:13, batch:25\n",
            "epoch:13, batch:30\n",
            "epoch:13, batch:35\n",
            "epoch:13, batch:40\n",
            "epoch:13, batch:45\n",
            "epoch:13, batch:50\n",
            "epoch:13, batch:55\n",
            "epoch:13, batch:60\n",
            "epoch:13, batch:65\n",
            "epoch:13, batch:70\n",
            "epoch:13, batch:75\n",
            "epoch:13, batch:80\n",
            "epoch:13, batch:85\n",
            "epoch:13, batch:90\n",
            "epoch:13, batch:95\n",
            "epoch:13, batch:100\n",
            "epoch:13, batch:105\n",
            "epoch:13, batch:110\n",
            "epoch:13, batch:115\n",
            "epoch:13, batch:120\n",
            "epoch:13, batch:125\n",
            "epoch:13, batch:130\n",
            "EarlyStopping counter: 2 out of 25\n",
            "Epoch 14: train_loss: 0.0061 train_acc: 91.2050 | val_loss: 0.0098 val_acc: 67.7734\n",
            "epoch:14, batch:0\n",
            "epoch:14, batch:5\n",
            "epoch:14, batch:10\n",
            "epoch:14, batch:15\n",
            "epoch:14, batch:20\n",
            "epoch:14, batch:25\n",
            "epoch:14, batch:30\n",
            "epoch:14, batch:35\n",
            "epoch:14, batch:40\n",
            "epoch:14, batch:45\n",
            "epoch:14, batch:50\n",
            "epoch:14, batch:55\n",
            "epoch:14, batch:60\n",
            "epoch:14, batch:65\n",
            "epoch:14, batch:70\n",
            "epoch:14, batch:75\n",
            "epoch:14, batch:80\n",
            "epoch:14, batch:85\n",
            "epoch:14, batch:90\n",
            "epoch:14, batch:95\n",
            "epoch:14, batch:100\n",
            "epoch:14, batch:105\n",
            "epoch:14, batch:110\n",
            "epoch:14, batch:115\n",
            "epoch:14, batch:120\n",
            "epoch:14, batch:125\n",
            "epoch:14, batch:130\n",
            "EarlyStopping counter: 3 out of 25\n",
            "Epoch 15: train_loss: 0.0060 train_acc: 91.8205 | val_loss: 0.0098 val_acc: 67.7734\n",
            "Saving model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "epochs = range(n_epochs)\n",
        "# train_acc_list\n",
        "# val_acc_list\n",
        "# train_loss_list\n",
        "# val_loss_list\n",
        "# plt.plot(epochs, train_acc_list)\n",
        "# plt.plot(epochs, val_acc_list)\n",
        "def plot_train_info(train_acc_list, val_acc_list,train_loss_list, val_loss_list):\n",
        "    fig1, ax1 = plt.subplots()\n",
        "    ax1.plot(epochs, train_acc_list, label=\"train acc\")\n",
        "    ax1.plot(epochs, val_acc_list, label=\"val acc\")\n",
        "    ax1.set_title(\"accuracy plot\")\n",
        "    ax1.set_xlabel(\"epochs\")\n",
        "    ax1.legend(loc=\"upper left\")\n",
        "    fig2, ax2 = plt.subplots()\n",
        "    ax2.plot(epochs, train_loss_list, label=\"train loss\")\n",
        "    ax2.plot(epochs, val_loss_list, label=\"val loss\")\n",
        "    ax2.set_title(\"loss plot\")\n",
        "    ax2.set_xlabel(\"epochs\")\n",
        "    ax2.legend(loc=\"upper left\")\n",
        "    \n",
        "plot_train_info(train_acc_list_cov, val_acc_list_cov,train_loss_list_cov, val_loss_list_cov)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "IW7va4LZIjof",
        "outputId": "7233acf1-fbb8-4b74-f0e5-4f0b479769cb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_harm_c = torch.load(\"/content/drive/MyDrive/CS19-1/Week8/stage2-2_FHM.pt\")"
      ],
      "metadata": {
        "id": "vSFalYgH9CNr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_harm_c.eval()"
      ],
      "metadata": {
        "id": "iaxmKDDy9Pxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluation(model, dataloader_test, test_samples_frame, model_name):\n",
        "    outputs,ls_truth = test_model(model, dataloader_test,test_samples_frame)\n",
        "    # Multiclass setting - Harmful\n",
        "    y_pred=[]\n",
        "    for i in outputs:\n",
        "    #     print(np.argmax(i))\n",
        "        y_pred.append(np.argmax(i))\n",
        "    # # np.argmax(outputs[:])\n",
        "    # outputs\n",
        "\n",
        "    # # Multiclass setting\n",
        "    aa = []\n",
        "    for batch in ls_truth:\n",
        "        aa+=batch\n",
        "\n",
        "    return y_pred, aa"
      ],
      "metadata": {
        "id": "hqezPYlfV-hD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr=0.0007\n",
        "# criterion = nn.BCELoss() #Binary case\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# # ------------Fresh training------------\n",
        "# model = MM(output_size)\n",
        "# model.to(device)\n",
        "# print(model_harm_p)"
      ],
      "metadata": {
        "id": "-7-QLzBqHBKe"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred,aa = model_evaluation(model_harm_c, dataloader_test, test_samples_frame_seen,\"Stage 2-2 FHM\")"
      ],
      "metadata": {
        "id": "M3RFGrk5JaCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f32874a-1c39-4366-a00a-8c967dc69253"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls = []\n",
        "for i in aa:\n",
        "    ls.append(i.item())\n",
        "    "
      ],
      "metadata": {
        "id": "0UF-xEzUQs-p"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "311HrOCJRhgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "rec = np.round(recall_score(ls, y_pred, average=\"macro\"),4)\n",
        "prec = np.round(precision_score(ls, y_pred, average=\"macro\"),4)\n",
        "f1 = np.round(f1_score(ls, y_pred, average=\"macro\"),4)\n",
        "auroc = np.round(roc_auc_score(ls, y_pred, average=\"macro\"),4)\n",
        "# hl = np.round(hamming_loss(test_labels, y_pred),4)\n",
        "acc = np.round(accuracy_score(ls, y_pred),4)\n",
        "# mae = np.round(mean_absolute_error(ls, y_pred),4)\n",
        "\n",
        "\n",
        "print(\"recall_score\\t: \",rec)\n",
        "print(\"precision_score\\t: \",prec)\n",
        "print(\"f1_score\\t: \",f1)\n",
        "print(\"acc\\t: \",acc)\n",
        "print(\"auroc\\t: \",auroc)\n",
        "# print(\"hamming_loss\\t: \",hl)\n",
        "# print(\"accuracy_score\\t: \",f1)\n",
        "print(classification_report(ls, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rVEaTlTQfR-",
        "outputId": "8266decb-966c-4af3-844c-cbd4b0a1a436"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall_score\t:  0.6681\n",
            "precision_score\t:  0.6816\n",
            "f1_score\t:  0.6629\n",
            "acc\t:  0.6698\n",
            "auroc\t:  0.6681\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.80      0.71       486\n",
            "           1       0.72      0.53      0.61       474\n",
            "\n",
            "    accuracy                           0.67       960\n",
            "   macro avg       0.68      0.67      0.66       960\n",
            "weighted avg       0.68      0.67      0.66       960\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_harm_c,\"stage2-2_FHM.pt\")"
      ],
      "metadata": {
        "id": "JbsQ66hJMXIT"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_size = 2\n",
        "exp_name = \"EMNLP_MCHarm_GLAREAll_POLTrain_POLEval\"\n",
        "# pre_trn_ckp = \"EMNLP_MCHarm_GLAREAll_COVTrain\" # Uncomment for using pre-trained\n",
        "exp_path = \"path_to_saved_files/EMNLP_ModelCkpt/\"+exp_name\n",
        "lr=0.0005\n",
        "# criterion = nn.BCELoss() #Binary case\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# # ------------Fresh training------------\n",
        "model_2_2_test = MM(output_size)\n",
        "model_2_2_test.to(device)\n",
        "# print(model_harm_p)\n",
        "optimizer = torch.optim.Adam(model_2_2_test.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "n_epochs = 5\n",
        "# early stopping patience; how long to wait after last time validation loss improved.\n",
        "patience = 25\n",
        "model_2_2_test, train_acc_list_cov, val_acc_list_cov, train_loss_list_cov, val_loss_list_cov, epoc_num = train_model(model_2_2_test, patience,\n",
        "                                                                                            n_epochs,dataloader_train_pol,\n",
        "                                                                                            dataloader_val_pol)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff7HFNRkyraC",
        "outputId": "6363b40b-8268-4da4-c037-22fc60df85c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0, batch:0\n",
            "epoch:0, batch:5\n",
            "Validation loss decreased (inf --> 0.010531).  Saving model ...\n",
            "Epoch 1: train_loss: 0.0107 train_acc: 58.0729 | val_loss: 0.0105 val_acc: 63.2812\n",
            "epoch:1, batch:0\n",
            "epoch:1, batch:5\n",
            "Validation loss decreased (0.010531 --> 0.010369).  Saving model ...\n",
            "Epoch 2: train_loss: 0.0107 train_acc: 54.6875 | val_loss: 0.0104 val_acc: 63.2812\n",
            "epoch:2, batch:0\n",
            "epoch:2, batch:5\n",
            "Validation loss decreased (0.010369 --> 0.010284).  Saving model ...\n",
            "Epoch 3: train_loss: 0.0103 train_acc: 57.8125 | val_loss: 0.0103 val_acc: 63.2812\n",
            "epoch:3, batch:0\n",
            "epoch:3, batch:5\n",
            "EarlyStopping counter: 1 out of 25\n",
            "Epoch 4: train_loss: 0.0096 train_acc: 65.8854 | val_loss: 0.0103 val_acc: 58.5938\n",
            "epoch:4, batch:0\n",
            "epoch:4, batch:5\n",
            "Validation loss decreased (0.010284 --> 0.010067).  Saving model ...\n",
            "Epoch 5: train_loss: 0.0087 train_acc: 77.0833 | val_loss: 0.0101 val_acc: 65.6250\n",
            "Saving model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred,aa = model_evaluation(model_2_2_test, dataloader_test_pol, test_samples_frame_muliti,\"Stage 2-2_multioff\")\n",
        "ls = []\n",
        "for i in aa:\n",
        "    ls.append(i.item())\n",
        "rec = np.round(recall_score(ls, y_pred, average=\"macro\"),4)\n",
        "prec = np.round(precision_score(ls, y_pred, average=\"macro\"),4)\n",
        "f1 = np.round(f1_score(ls, y_pred, average=\"macro\"),4)\n",
        "# hl = np.round(hamming_loss(test_labels, y_pred),4)\n",
        "acc = np.round(accuracy_score(ls, y_pred),4)\n",
        "mae = np.round(mean_absolute_error(ls, y_pred),4)\n",
        "print(\"recall_score\\t: \",rec)\n",
        "print(\"precision_score\\t: \",prec)\n",
        "print(\"f1_score\\t: \",f1)\n",
        "# print(\"hamming_loss\\t: \",hl)\n",
        "# print(\"accuracy_score\\t: \",f1)\n",
        "print(classification_report(ls, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBoejnY50A4O",
        "outputId": "17c08eba-e98f-4242-e580-c45087663ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall_score\t:  0.5893\n",
            "precision_score\t:  0.5905\n",
            "f1_score\t:  0.5897\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.69      0.68        77\n",
            "           1       0.51      0.49      0.50        51\n",
            "\n",
            "    accuracy                           0.61       128\n",
            "   macro avg       0.59      0.59      0.59       128\n",
            "weighted avg       0.61      0.61      0.61       128\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_best = torch.load(\"/content/drive/MyDrive/CS19-1/Week8/stage2-2best.pt\")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "n6QIs3Jdlaw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred,aa = model_evaluation(model_best, dataloader_test_cov, test_samples_frame_cov,\"Stage 2-2\")\n",
        "ls = []\n",
        "for i in aa:\n",
        "    ls.append(i.item())\n",
        "rec = np.round(recall_score(ls, y_pred, average=\"macro\"),4)\n",
        "prec = np.round(precision_score(ls, y_pred, average=\"macro\"),4)\n",
        "f1 = np.round(f1_score(ls, y_pred, average=\"macro\"),4)\n",
        "# hl = np.round(hamming_loss(test_labels, y_pred),4)\n",
        "acc = np.round(accuracy_score(ls, y_pred),4)\n",
        "mae = np.round(mean_absolute_error(ls, y_pred),4)\n",
        "print(\"acc_score\\t: \",acc)\n",
        "print(\"recall_score\\t: \",rec)\n",
        "print(\"precision_score\\t: \",prec)\n",
        "print(\"f1_score\\t: \",f1)\n",
        "# print(\"hamming_loss\\t: \",hl)\n",
        "# print(\"accuracy_score\\t: \",f1)\n",
        "print(classification_report(ls, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0QG8viBlfds",
        "outputId": "ebdcbc24-da32-4163-a5b6-1bde8f64804d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc_score\t:  0.825\n",
            "recall_score\t:  0.8408\n",
            "precision_score\t:  0.8236\n",
            "f1_score\t:  0.8225\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.77      0.84       196\n",
            "           1       0.72      0.91      0.80       124\n",
            "\n",
            "    accuracy                           0.82       320\n",
            "   macro avg       0.82      0.84      0.82       320\n",
            "weighted avg       0.85      0.82      0.83       320\n",
            "\n"
          ]
        }
      ]
    }
  ]
}