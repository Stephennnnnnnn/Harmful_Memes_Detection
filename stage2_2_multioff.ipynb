{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD9OoA5nnjbf",
        "outputId": "0dc57a41-7cb9-4db8-e9bc-38511032ba77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "!pip install ftfy\n",
        "\n",
        "# Authenticate\n",
        "drive = None\n",
        "def authenticate():\n",
        "    global drive\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "\n",
        "#Download files\n",
        "def downloadFiles(fileIds):\n",
        "    authenticate()\n",
        "    for fileId in fileIds:    \n",
        "        downloaded = drive.CreateFile({\"id\": fileId[1]})\n",
        "        downloaded.GetContentFile(fileId[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download file if not existing\n",
        "try:\n",
        "  _ = open(\"bpe_simple_vocab_16e6.txt.gz\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"bpe_simple_vocab_16e6.txt.gz\", \"1sazrmZm-bsAyLap-kvFCVn8uXdGppVo-\"]])\n",
        "\n",
        "try:\n",
        "  _ = open(\"clip_model.pt\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"clip_model.pt\", \"1UO7E2nu_4-C5RRcTtYKqCRjLPUtzV2NQ\"]])\n",
        "\n",
        "\n",
        "try:\n",
        "  _ = open(\"Split_Dataset.zip\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"Split_Dataset.zip\", \"1goTilek47u98r0DhqiEIGWkOKL5ZpVws\"]])\n",
        "  \n",
        "try:\n",
        "  _ = open(\"Labelled_Images.zip\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"Labelled_Images.zip\", \"1NoSdyL0KAHuDY40uEUoDpERmeQ0X7m-z\"]])\n",
        "\n",
        "import zipfile\n",
        "f = zipfile.ZipFile(\"Split_Dataset.zip\", \"r\")\n",
        "for file in f.namelist():\n",
        "    f.extract(file, \"./\")\n",
        "f.close()\n",
        "\n",
        "f = zipfile.ZipFile(\"Labelled_Images.zip\", \"r\")\n",
        "for file in f.namelist():\n",
        "    f.extract(file, \"./\")\n",
        "f.close()"
      ],
      "metadata": {
        "id": "8VXH9xYxnt-8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "# from torchnlp import encoders\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import  mean_absolute_error\n",
        "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
        "from pathlib import Path\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "import gzip\n",
        "import html\n",
        "import os\n",
        "from functools import lru_cache\n",
        "\n",
        "import ftfy\n",
        "import regex as re\n",
        "!pip install kornia\n",
        "from kornia.filters import gaussian_blur2d"
      ],
      "metadata": {
        "id": "iX8Sr6-4n46c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e30a743-d514-477f-dccc-d8841f9b9f1c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kornia\n",
            "  Downloading kornia-0.6.7-py2.py3-none-any.whl (565 kB)\n",
            "\u001b[K     |████████████████████████████████| 565 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from kornia) (21.3)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from kornia) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->kornia) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->kornia) (3.0.9)\n",
            "Installing collected packages: kornia\n",
            "Successfully installed kornia-0.6.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = 'Labelled Images'\n",
        "train_path = 'Split Dataset/Training_meme_dataset.csv'\n",
        "dev_path = \"Split Dataset/Validation_meme_dataset.csv\"\n",
        "test_path = \"Split Dataset/Testing_meme_dataset.csv\"\n",
        "\n",
        "test_sample = pd.read_csv(test_path)"
      ],
      "metadata": {
        "id": "xwEQWBCLrNLk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))"
      ],
      "metadata": {
        "id": "9MXXjmG6MyXq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def basic_clean(text):\n",
        "    text = ftfy.fix_text(text)\n",
        "    text = html.unescape(html.unescape(text))\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def whitespace_clean(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "class SimpleTokenizer(object):\n",
        "    def __init__(self, bpe_path: str = \"bpe_simple_vocab_16e6.txt.gz\"):\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
        "        merges = merges[1:49152-256-2+1]\n",
        "        merges = [tuple(merge.split()) for merge in merges]\n",
        "        vocab = list(bytes_to_unicode().values())\n",
        "        vocab = vocab + [v+'</w>' for v in vocab]\n",
        "        for merge in merges:\n",
        "            vocab.append(''.join(merge))\n",
        "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
        "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
        "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token+'</w>'\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                    new_word.append(first+second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = ' '.join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        text = whitespace_clean(basic_clean(text)).lower()\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = ''.join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
        "        return text"
      ],
      "metadata": {
        "id": "OlLcdtmsM1l0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os, sys\n",
        "# sys.path.append('path_to_the_module/early-stopping-pytorch')\n",
        "\n",
        "# 克服 EarlyStopping 问题 - https://github.com/Bjarten/early-stopping-pytorch - 下载py文件到本地\n",
        "# from pytorchtools import EarlyStopping\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "metadata": {
        "id": "tIDcTEhDN9aC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model = torch.jit.load(\"clip_model.pt\").cuda().eval()\n",
        "input_resolution = clip_model.input_resolution.item()\n",
        "context_length = clip_model.context_length.item()\n",
        "vocab_size = clip_model.vocab_size.item()"
      ],
      "metadata": {
        "id": "BlYDBL_8rgAd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = Compose([\n",
        "    Resize(input_resolution, interpolation=Image.BICUBIC),\n",
        "    CenterCrop(input_resolution),\n",
        "    ToTensor()\n",
        "    ])\n",
        "tokenizer = SimpleTokenizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcfLE6V6rhmS",
        "outputId": "b4d0fb71-eda3-49c2-acc1-3df749f3631f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
            "  \"Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the image features for a single image input\n",
        "def process_image_clip(in_img):\n",
        "    image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
        "    image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()\n",
        "    \n",
        "    image = preprocess(Image.open(in_img).convert(\"RGB\"))\n",
        "    \n",
        "    image_input = torch.tensor(np.stack(image)).cuda()\n",
        "    image_input -= image_mean[:, None, None]\n",
        "    image_input /= image_std[:, None, None]\n",
        "    return image_input\n",
        "\n",
        "# Get the text features for a single text input\n",
        "def process_text_clip(in_text):    \n",
        "    text_token = tokenizer.encode(in_text)\n",
        "    text_input = torch.zeros(clip_model.context_length, dtype=torch.long)\n",
        "    sot_token = tokenizer.encoder['<|startoftext|>']\n",
        "    eot_token = tokenizer.encoder['<|endoftext|>']\n",
        "    tokens = [sot_token] + text_token[:75] + [eot_token]\n",
        "    text_input[:len(tokens)] = torch.tensor(tokens)\n",
        "    text_input = text_input.cuda()\n",
        "    return text_input"
      ],
      "metadata": {
        "id": "pNaaZaUyrj1A"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "R5lz3mmHNdXV"
      },
      "outputs": [],
      "source": [
        "class meme_data(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_path,\n",
        "        img_dir,\n",
        "        split_flag=None,\n",
        "        balance=False,\n",
        "        dev_limit=None,\n",
        "        random_state=0,\n",
        "    ):\n",
        "\n",
        "        self.samples_frame = pd.read_csv(\n",
        "            data_path\n",
        "        )\n",
        "        self.samples_frame = self.samples_frame.reset_index(\n",
        "            drop=True\n",
        "        )\n",
        "        self.samples_frame.image_name = self.samples_frame.apply(\n",
        "            lambda row: (img_dir + '/' + row.image_name), axis=1\n",
        "        )\n",
        "\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.samples_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_file_name = self.samples_frame.loc[idx, \"image_name\"]\n",
        "        \n",
        "        image_clip_input = process_image_clip(self.samples_frame.loc[idx, \"image_name\"])      \n",
        "#         Pre-extracted features  \n",
        "\n",
        "        text_clip_input = process_text_clip(self.samples_frame.loc[idx, \"sentence\"])\n",
        "#         -------------------------------------------------------------------------------\n",
        "\n",
        "        if self.samples_frame.loc[idx, \"label\"]==\"Non-offensiv\":\n",
        "            lab=0\n",
        "        else:\n",
        "            lab=1\n",
        "        label = torch.tensor(lab).to(device)  \n",
        "\n",
        "        \n",
        "        sample = {\n",
        "            \"image_clip_input\": image_clip_input,\n",
        "            \"text_clip_input\": text_clip_input,\n",
        "            \"label\": label\n",
        "        }\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_hm = meme_data(train_path, image_dir, split_flag='train')\n",
        "dataloader_train_pol = DataLoader(train_data_hm, batch_size=64,shuffle=True, num_workers=0)\n",
        "\n",
        "dev_data_hm = meme_data(dev_path, image_dir, split_flag='val')\n",
        "dataloader_val_pol = DataLoader(dev_data_hm, batch_size=64,shuffle=True, num_workers=0)\n",
        "\n",
        "test_data_hm = meme_data(test_path, image_dir, split_flag='test')\n",
        "dataloader_test_pol = DataLoader(test_data_hm, batch_size=64, shuffle=False, num_workers=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "VcKqmn8JKaDJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SinkhornSolver(nn.Module):\n",
        "    \"\"\"\n",
        "    Optimal Transport solver under entropic regularisation.\n",
        "    Based on the code of Gabriel Peyré.\n",
        "    \"\"\"\n",
        "    def __init__(self, epsilon, iterations=100, ground_metric=lambda x: torch.pow(x, 2)):\n",
        "        super(SinkhornSolver, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.iterations = iterations\n",
        "        self.ground_metric = ground_metric\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        num_x = x.size(-2)\n",
        "        num_y = y.size(-2)\n",
        "        \n",
        "        batch_size = 1 if x.dim() == 2 else x.size(0)\n",
        "\n",
        "        # Marginal densities are empirical measures\n",
        "        a = x.new_ones((batch_size, num_x), requires_grad=False) / num_x\n",
        "        b = y.new_ones((batch_size, num_y), requires_grad=False) / num_y\n",
        "        \n",
        "        a = a.squeeze()\n",
        "        b = b.squeeze()\n",
        "                \n",
        "        # Initialise approximation vectors in log domain\n",
        "        u = torch.zeros_like(a)\n",
        "        v = torch.zeros_like(b)\n",
        "\n",
        "        # Stopping criterion\n",
        "        threshold = 1e-1\n",
        "        \n",
        "        # Cost matrix\n",
        "        C = self._compute_cost(x, y)\n",
        "        \n",
        "        # Sinkhorn iterations\n",
        "        for i in range(self.iterations): \n",
        "            u0, v0 = u, v\n",
        "                        \n",
        "            # u^{l+1} = a / (K v^l)\n",
        "            K = self._log_boltzmann_kernel(u, v, C)\n",
        "            u_ = torch.log(a + 1e-8) - torch.logsumexp(K, dim=1)\n",
        "            u = self.epsilon * u_ + u\n",
        "                        \n",
        "            # v^{l+1} = b / (K^T u^(l+1))\n",
        "            K_t = self._log_boltzmann_kernel(u, v, C).transpose(-2, -1)\n",
        "            v_ = torch.log(b + 1e-8) - torch.logsumexp(K_t, dim=1)\n",
        "            v = self.epsilon * v_ + v\n",
        "            \n",
        "            # Size of the change we have performed on u\n",
        "            diff = torch.sum(torch.abs(u - u0), dim=-1) + torch.sum(torch.abs(v - v0), dim=-1)\n",
        "            mean_diff = torch.mean(diff)\n",
        "                        \n",
        "            if mean_diff.item() < threshold:\n",
        "                break\n",
        "   \n",
        "        # print(\"Finished computing transport plan in {} iterations\".format(i))\n",
        "    \n",
        "        # Transport plan pi = diag(a)*K*diag(b)\n",
        "        K = self._log_boltzmann_kernel(u, v, C)\n",
        "        pi = torch.exp(K)\n",
        "        \n",
        "        # Sinkhorn distance\n",
        "        cost = torch.sum(pi * C, dim=(-2, -1))\n",
        "\n",
        "        return cost, pi\n",
        "\n",
        "    def _compute_cost(self, x, y):\n",
        "        x_ = x.unsqueeze(-2)\n",
        "        y_ = y.unsqueeze(-3)\n",
        "        C = torch.sum(self.ground_metric(x_ - y_), dim=-1)\n",
        "        return C\n",
        "\n",
        "    def _log_boltzmann_kernel(self, u, v, C=None):\n",
        "        C = self._compute_cost(x, y) if C is None else C\n",
        "        kernel = -C + u.unsqueeze(-1) + v.unsqueeze(-2)\n",
        "        kernel /= self.epsilon\n",
        "        return kernel\n",
        "\n",
        "def sinkhorn_iter(x,y):\n",
        "    epsilon = 10**(-(2))\n",
        "    solver = SinkhornSolver(epsilon=epsilon, iterations=10000)\n",
        "    cost, pi = solver.forward(x, y)\n",
        "    return cost,pi"
      ],
      "metadata": {
        "id": "d3jYr15Eg1D7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the model"
      ],
      "metadata": {
        "id": "CGZXIsXrMsmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "5ZL13U_2oGwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aa = nn.Linear(64, 2).to(device)\n",
        "for data in dataloader_train_pol:\n",
        "    a = data[\"image_clip_input\"]\n",
        "    b = data[\"text_clip_input\"]\n",
        "    img_feat_clip = clip_model.encode_image(a).float().to(device)\n",
        "    txt_feat_clip = clip_model.encode_text(b).float().to(device)\n",
        "    in_CI_smo = gaussian_blur2d(img_feat_clip.unsqueeze(0).unsqueeze(0),(3, 3), (1.5, 1.5)) #[1, 1, 64, 512]\n",
        "    in_CT_smo = gaussian_blur2d(txt_feat_clip.unsqueeze(0).unsqueeze(0),(3, 3), (1.5, 1.5)) #[1, 1, 64, 512]\n",
        "    cost,pi = sinkhorn_iter(in_CI_smo.squeeze(0).squeeze(0),in_CT_smo.squeeze(0).squeeze(0))\n",
        "    print(F.relu(aa(pi)))\n",
        "    break"
      ],
      "metadata": {
        "id": "kwymCSemPgvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = nn.Linear(64, 2).to(device)\n",
        "F.sigmoid(a(pi))"
      ],
      "metadata": {
        "id": "s65qhNmcPdi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the cross attention value features \n",
        "# Vanilla model\n",
        "\n",
        "class MM(nn.Module):\n",
        "    def __init__(self, n_out):\n",
        "        super(MM, self).__init__()  \n",
        "        self.dense_vgg_1024 = nn.Linear(4096, 1024)\n",
        "        self.dense_vgg_512 = nn.Linear(1024, 512)\n",
        "        self.drop20 = nn.Dropout(p=0.2)\n",
        "        self.drop5 = nn.Dropout(p=0.05) \n",
        "        \n",
        "        self.dense_drob_512 = nn.Linear(768, 512)\n",
        "        \n",
        "        self.gen_key_L1 = nn.Linear(512, 256) # 512X256\n",
        "        self.gen_query_L1 = nn.Linear(512, 256) # 512X256\n",
        "        self.gen_key_L2 = nn.Linear(512, 256) # 512X256\n",
        "        self.gen_query_L2 = nn.Linear(512, 256) # 512X256\n",
        "        self.gen_key_L3 = nn.Linear(512, 256) # 512X256\n",
        "        self.gen_query_L3 = nn.Linear(512, 256) # 512X256\n",
        "#         self.gen_value = nn.Linear(512, 256) # 512X256\n",
        "        self.soft = nn.Softmax(dim=1)\n",
        "        self.soft_final = nn.Softmax(dim=1)\n",
        "        self.project_dense_512a = nn.Linear(1024, 512) # 512X256\n",
        "        self.project_dense_512b = nn.Linear(1024, 512) # 512X256\n",
        "        self.project_dense_512c = nn.Linear(1024, 512) # 512X256 \n",
        "        \n",
        "        \n",
        "        self.fc_out = nn.Linear(512, 256) # 512X256\n",
        "        self.sink_out = nn.Linear(64, 2) # 512X256\n",
        "        self.out = nn.Linear(256, n_out) # 512X256\n",
        "        \n",
        "    def selfattNFuse(self, vec1, vec2): \n",
        "            q1 = F.relu(self.gen_query_L1(vec1))\n",
        "            k1 = F.relu(self.gen_key_L1(vec1))\n",
        "            q2 = F.relu(self.gen_query_L1(vec2))\n",
        "            k2 = F.relu(self.gen_key_L1(vec2))\n",
        "            score1 = torch.reshape(torch.bmm(q1.view(-1, 1, 256), k2.view(-1, 256, 1)), (-1, 1))\n",
        "            score2 = torch.reshape(torch.bmm(q2.view(-1, 1, 256), k1.view(-1, 256, 1)), (-1, 1))\n",
        "            wt_score1_score2_mat = torch.cat((score1, score2), 1)\n",
        "            wt_i1_i2 = self.soft(wt_score1_score2_mat.float()) #prob\n",
        "            prob_1 = wt_i1_i2[:,0]\n",
        "            prob_2 = wt_i1_i2[:,1]\n",
        "            wtd_i1 = vec1 * prob_1[:, None]\n",
        "            wtd_i2 = vec2 * prob_2[:, None]\n",
        "            out_rep = F.relu(self.project_dense_512a(torch.cat((wtd_i1,wtd_i2), 1)))\n",
        "            return out_rep\n",
        "  \n",
        "\n",
        "\n",
        "    def forward(self, in_CI, in_CT):\n",
        "        # print(in_CI.shape,in_CT.shape)\n",
        "        in_CI_smo = gaussian_blur2d(in_CI.unsqueeze(0).unsqueeze(0),(3, 3), (1.5, 1.5)) #[1, 1, 64, 512]\n",
        "        in_CT_smo = gaussian_blur2d(in_CT.unsqueeze(0).unsqueeze(0),(3, 3), (1.5, 1.5)) #[1, 1, 64, 512]\n",
        "\n",
        "        cost,pi = sinkhorn_iter(in_CI_smo.squeeze(0).squeeze(0),in_CT_smo.squeeze(0).squeeze(0)) #64x64\n",
        "        score_sink = F.sigmoid(self.sink_out(pi)) #64x2\n",
        "        # print(score_sink)\n",
        "        res_inter = self.selfattNFuse(in_CI, in_CT) #64x512\n",
        "        res_inter_score = F.relu(self.fc_out(res_inter)) #64x256\n",
        "        res_inter_score_final = torch.sigmoid(self.out(res_inter_score)) #64x2\n",
        "        score_sum = res_inter_score_final + score_sink\n",
        "        return score_sum"
      ],
      "metadata": {
        "id": "9wlXGmNlN20i"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training function\n"
      ],
      "metadata": {
        "id": "xJMN8x1BMv0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For cross entropy loss\n",
        "def train_model(model, patience, n_epochs, dataloader_train, dataloader_val):\n",
        "    epochs = n_epochs\n",
        "#     clip = 5\n",
        "\n",
        "    train_acc_list=[]\n",
        "    val_acc_list=[]\n",
        "    train_loss_list=[]\n",
        "    val_loss_list=[]\n",
        "    \n",
        "        # initialize the experiment path\n",
        "    Path(exp_path).mkdir(parents=True, exist_ok=True)\n",
        "    # initialize early_stopping object\n",
        "    chk_file = os.path.join(exp_path, 'checkpoint_'+exp_name+'.pt')\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True, path=chk_file)\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    for i in range(epochs):\n",
        "#         total_acc_train = 0\n",
        "        total_loss_train = 0\n",
        "        total_train = 0\n",
        "        correct_train = 0\n",
        "\n",
        "        for ii,data in enumerate(dataloader_train):\n",
        "            \n",
        "#             Clip features...\n",
        "            img_inp_clip = data['image_clip_input']\n",
        "            txt_inp_clip = data['text_clip_input']\n",
        "            # bert_in = data['text_drob_feature']\n",
        "            with torch.no_grad():\n",
        "                img_feat_clip = clip_model.encode_image(img_inp_clip).float().to(device)\n",
        "                txt_feat_clip = clip_model.encode_text(txt_inp_clip).float().to(device)\n",
        "                # global_img = global_feature[0].float().to(device)\n",
        "                # global_txt = global_feature[1].float().to(device)\n",
        "\n",
        "            label_train = data['label'].to(device)\n",
        "\n",
        "            model.zero_grad()\n",
        "            try:\n",
        "                output = model(img_feat_clip, txt_feat_clip)\n",
        "                # print(output.squeeze())\n",
        "                loss = criterion(output.squeeze(), label_train)\n",
        "                \n",
        "                loss.backward()\n",
        "    #             nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "                optimizer.step()\n",
        "            \n",
        "\n",
        "                with torch.no_grad():\n",
        "                    _, predicted_train = torch.max(output.data, 1)\n",
        "                    total_train += label_train.size(0)\n",
        "                    correct_train += (predicted_train == label_train).sum().item()\n",
        "    #                 out_val = (output.squeeze()>0.5).float()\n",
        "    #                 out_final = ((out_val == 1).nonzero(as_tuple=True)[0])\n",
        "    #                 print()\n",
        "    #                 acc = torch.abs(output.squeeze() - label.float()).view(-1)\n",
        "    #                 acc = (1. - acc.sum() / acc.size()[0])\n",
        "    #                 total_acc_train += acc\n",
        "                    total_loss_train += loss.item()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            if ii%5 == 0:\n",
        "                print(f\"epoch:{i}, batch:{ii}\")\n",
        "\n",
        "        # print(label_train,correct_train,total_train)\n",
        "        train_acc = 100 * correct_train / total_train\n",
        "        train_loss = total_loss_train/total_train\n",
        "        model.eval()\n",
        "#         total_acc_val = 0\n",
        "        total_loss_val = 0\n",
        "        total_val = 0\n",
        "        correct_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in dataloader_val:                \n",
        "#                 Clip features...                \n",
        "                img_inp_clip = data['image_clip_input']\n",
        "                txt_inp_clip = data['text_clip_input']\n",
        "                # bert_in = data['text_drob_feature']\n",
        "                with torch.no_grad():\n",
        "                    img_feat_clip = clip_model.encode_image(img_inp_clip).float().to(device)\n",
        "                    txt_feat_clip = clip_model.encode_text(txt_inp_clip).float().to(device)\n",
        "                    # global_img = global_feature[0].float().to(device)\n",
        "                    # global_txt = global_feature[1].float().to(device)\n",
        "        \n",
        "                label_val = data['label'].to(device)\n",
        "\n",
        "                model.zero_grad()\n",
        "                try:\n",
        "                    output = model(img_feat_clip, txt_feat_clip)\n",
        "                    \n",
        "                    \n",
        "                    val_loss = criterion(output.squeeze(), label_val)\n",
        "                    _, predicted_val = torch.max(output.data, 1)\n",
        "                    total_val += label_val.size(0)\n",
        "                    correct_val += (predicted_val == label_val).sum().item()                \n",
        "                    total_loss_val += val_loss.item()\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        if i == 10:\n",
        "            print(\"Saving model...\") \n",
        "            torch.save(model.state_dict(), os.path.join(exp_path, \"10epoch_model.pt\"))\n",
        "        val_acc = 100 * correct_val / total_val\n",
        "        val_loss = total_loss_val/total_val\n",
        "\n",
        "        train_acc_list.append(train_acc)\n",
        "        val_acc_list.append(val_acc)\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_loss_list.append(val_loss)\n",
        "        \n",
        "        early_stopping(val_loss, model)\n",
        "        \n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "            \n",
        "\n",
        "        print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
        "        model.train()\n",
        "        torch.cuda.empty_cache()\n",
        "    print(\"Saving model...\") \n",
        "    torch.save(model.state_dict(), os.path.join(exp_path, \"final_model.pt\"))\n",
        "    # load the last checkpoint with the best model\n",
        "#     model.load_state_dict(torch.load('checkpoint_1.pt'))\n",
        "    \n",
        "    return  model, train_acc_list, val_acc_list, train_loss_list, val_loss_list, i"
      ],
      "metadata": {
        "id": "oVB4mHJ0OBPF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For CE loss\n",
        "def test_model(model, dataloader_test,test_frame):\n",
        "    ls_truth = []\n",
        "    model.eval()\n",
        "    total_test = 0\n",
        "    correct_test =0\n",
        "    total_acc_test = 0\n",
        "    total_loss_test = 0\n",
        "    outputs = []\n",
        "    test_labels=[]\n",
        "    with torch.no_grad():\n",
        "        for i,data in enumerate(dataloader_test):\n",
        "            img_inp_clip = data['image_clip_input']\n",
        "            txt_inp_clip = data['text_clip_input']\n",
        "            # bert_in = data['text_drob_feature']\n",
        "            with torch.no_grad():\n",
        "                img_feat_clip = clip_model.encode_image(img_inp_clip).float().to(device)\n",
        "                txt_feat_clip = clip_model.encode_text(txt_inp_clip).float().to(device)\n",
        "                # global_feature = clip_model(img_inp_clip,txt_inp_clip)\n",
        "                # global_img = global_feature[0].float().to(device)\n",
        "                # global_txt = global_feature[1].float().to(device)          \n",
        "\n",
        "            label_test = data['label'].to(device)\n",
        "            \n",
        "#             out = model(img_feat_vgg, txt_feat_trans)        \n",
        "            try:\n",
        "                out = model(img_feat_clip, txt_feat_clip)        \n",
        "\n",
        "                outputs += list(out.cpu().data.numpy())\n",
        "                loss = criterion(out.squeeze(), label_test)\n",
        "                \n",
        "                _, predicted_test = torch.max(out.data, 1)\n",
        "                total_test += label_test.size(0)\n",
        "                correct_test += (predicted_test == label_test).sum().item()\n",
        "    #                 out_val = (output.squeeze()>0.5).float()\n",
        "    #                 out_final = ((out_val == 1).nonzero(as_tuple=True)[0])\n",
        "    #                 print()\n",
        "    #                 acc = torch.abs(output.squeeze() - label.float()).view(-1)\n",
        "    #                 acc = (1. - acc.sum() / acc.size()[0])\n",
        "    #                 total_acc_train += acc\n",
        "                ls_truth.append(label_test)\n",
        "            except:\n",
        "                pass\n",
        "            \n",
        "            \n",
        "#     #         print(label.float())\n",
        "#             acc = torch.abs(out.squeeze() - label.float()).view(-1)\n",
        "#     #         print((acc.sum() / acc.size()[0]))\n",
        "#             acc = (1. - acc.sum() / acc.size()[0])\n",
        "#     #         print(acc)\n",
        "#             total_acc_test += acc\n",
        "#             total_loss_test += loss.item()\n",
        "\n",
        "    \n",
        "    acc_test = 100 * correct_test / total_test\n",
        "    loss_test = total_loss_test/total_test   \n",
        "\n",
        "    return outputs,ls_truth\n"
      ],
      "metadata": {
        "id": "AsplvOxzOIkN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "MIcLmqDwN3_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_size = 2\n",
        "exp_name = \"EMNLP_MCHarm_GLAREAll_POLTrain_POLEval\"\n",
        "# pre_trn_ckp = \"EMNLP_MCHarm_GLAREAll_COVTrain\" # Uncomment for using pre-trained\n",
        "exp_path = \"path_to_saved_files/EMNLP_ModelCkpt/\"+exp_name\n",
        "lr=0.0005\n",
        "# criterion = nn.BCELoss() #Binary case\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# # ------------Fresh training------------\n",
        "model = MM(output_size)\n",
        "model.to(device)\n",
        "# print(model_harm_p)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "n_epochs = 3\n",
        "# early stopping patience; how long to wait after last time validation loss improved.\n",
        "patience = 25\n",
        "model_harm_c, train_acc_list_cov, val_acc_list_cov, train_loss_list_cov, val_loss_list_cov, epoc_num = train_model(model, patience,\n",
        "                                                                                            n_epochs,dataloader_train_pol,\n",
        "                                                                                            dataloader_val_pol)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGuZOSQOKO3U",
        "outputId": "8745f032-fc26-47bf-f922-4b8677637e5d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0, batch:0\n",
            "epoch:0, batch:5\n",
            "Validation loss decreased (inf --> 0.010575).  Saving model ...\n",
            "Epoch 1: train_loss: 0.0108 train_acc: 55.9896 | val_loss: 0.0106 val_acc: 60.9375\n",
            "epoch:1, batch:0\n",
            "epoch:1, batch:5\n",
            "Validation loss decreased (0.010575 --> 0.010247).  Saving model ...\n",
            "Epoch 2: train_loss: 0.0103 train_acc: 61.1979 | val_loss: 0.0102 val_acc: 64.0625\n",
            "epoch:2, batch:0\n",
            "epoch:2, batch:5\n",
            "Validation loss decreased (0.010247 --> 0.010103).  Saving model ...\n",
            "Epoch 3: train_loss: 0.0093 train_acc: 75.5208 | val_loss: 0.0101 val_acc: 62.5000\n",
            "Saving model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_harm_c,\"stage2-2_f182.pt\")"
      ],
      "metadata": {
        "id": "qcYvXfDTxXoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 3\n",
        "# early stopping patience; how long to wait after last time validation loss improved.\n",
        "patience = 10\n",
        "model_harm_c, train_acc_list_cov, val_acc_list_cov, train_loss_list_cov, val_loss_list_cov, epoc_num = train_model(model_harm_c, patience,\n",
        "                                                                                            n_epochs,dataloader_train_pol,\n",
        "                                                                                            dataloader_val_pol)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6Pe8C1GVWJl",
        "outputId": "12313e72-482d-47e4-ca5e-47cd47af17d4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0, batch:0\n",
            "epoch:0, batch:5\n",
            "epoch:0, batch:10\n",
            "epoch:0, batch:15\n",
            "epoch:0, batch:20\n",
            "epoch:0, batch:25\n",
            "epoch:0, batch:30\n",
            "epoch:0, batch:35\n",
            "epoch:0, batch:40\n",
            "epoch:0, batch:45\n",
            "Validation loss decreased (inf --> 0.010122).  Saving model ...\n",
            "Epoch 1: train_loss: 0.0061 train_acc: 92.6197 | val_loss: 0.0101 val_acc: 65.6250\n",
            "epoch:1, batch:0\n",
            "epoch:1, batch:5\n",
            "epoch:1, batch:10\n",
            "epoch:1, batch:15\n",
            "epoch:1, batch:20\n",
            "epoch:1, batch:25\n",
            "epoch:1, batch:30\n",
            "epoch:1, batch:35\n",
            "epoch:1, batch:40\n",
            "epoch:1, batch:45\n",
            "Validation loss decreased (0.010122 --> 0.009963).  Saving model ...\n",
            "Epoch 2: train_loss: 0.0059 train_acc: 93.2181 | val_loss: 0.0100 val_acc: 67.1875\n",
            "epoch:2, batch:0\n",
            "epoch:2, batch:5\n",
            "epoch:2, batch:10\n",
            "epoch:2, batch:15\n",
            "epoch:2, batch:20\n",
            "epoch:2, batch:25\n",
            "epoch:2, batch:30\n",
            "epoch:2, batch:35\n",
            "epoch:2, batch:40\n",
            "epoch:2, batch:45\n",
            "Validation loss decreased (0.009963 --> 0.008884).  Saving model ...\n",
            "Epoch 3: train_loss: 0.0059 train_acc: 93.8830 | val_loss: 0.0089 val_acc: 75.0000\n",
            "Saving model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_harm_c,\"stage2-2best.pt\")"
      ],
      "metadata": {
        "id": "V47C8WRYJy8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluation(model, dataloader_test, test_samples_frame, model_name):\n",
        "    outputs,ls_truth = test_model(model, dataloader_test,test_samples_frame)\n",
        "    # Multiclass setting - Harmful\n",
        "    y_pred=[]\n",
        "    for i in outputs:\n",
        "    #     print(np.argmax(i))\n",
        "        y_pred.append(np.argmax(i))\n",
        "    # # np.argmax(outputs[:])\n",
        "    # outputs\n",
        "\n",
        "    # # Multiclass setting\n",
        "    aa = []\n",
        "    for batch in ls_truth:\n",
        "        aa+=batch\n",
        "\n",
        "    return y_pred, aa"
      ],
      "metadata": {
        "id": "hqezPYlfV-hD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples_frame_muliti = pd.read_csv(test_path)"
      ],
      "metadata": {
        "id": "7EZuZWx5KV0j"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred,aa = model_evaluation(model_harm_c, dataloader_test_pol, test_samples_frame_muliti,\"Stage 2-2\")"
      ],
      "metadata": {
        "id": "M3RFGrk5JaCz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls = []\n",
        "for i in aa:\n",
        "    ls.append(i.item())\n",
        "    "
      ],
      "metadata": {
        "id": "0UF-xEzUQs-p"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "311HrOCJRhgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rec = np.round(recall_score(ls, y_pred, average=\"macro\"),4)\n",
        "prec = np.round(precision_score(ls, y_pred, average=\"macro\"),4)\n",
        "f1 = np.round(f1_score(ls, y_pred, average=\"macro\"),4)\n",
        "# hl = np.round(hamming_loss(test_labels, y_pred),4)\n",
        "acc = np.round(accuracy_score(ls, y_pred),4)\n",
        "mae = np.round(mean_absolute_error(ls, y_pred),4)\n",
        "print(\"recall_score\\t: \",rec)\n",
        "print(\"precision_score\\t: \",prec)\n",
        "print(\"f1_score\\t: \",f1)\n",
        "# print(\"hamming_loss\\t: \",hl)\n",
        "# print(\"accuracy_score\\t: \",f1)\n",
        "print(classification_report(ls, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rVEaTlTQfR-",
        "outputId": "aa36ade3-c847-4088-b6bf-db1a1d597472"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall_score\t:  0.5695\n",
            "precision_score\t:  0.5756\n",
            "f1_score\t:  0.5694\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.73      0.69        77\n",
            "           1       0.50      0.41      0.45        51\n",
            "\n",
            "    accuracy                           0.60       128\n",
            "   macro avg       0.58      0.57      0.57       128\n",
            "weighted avg       0.59      0.60      0.59       128\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_size = 2\n",
        "exp_name = \"EMNLP_MCHarm_GLAREAll_POLTrain_POLEval\"\n",
        "# pre_trn_ckp = \"EMNLP_MCHarm_GLAREAll_COVTrain\" # Uncomment for using pre-trained\n",
        "exp_path = \"path_to_saved_files/EMNLP_ModelCkpt/\"+exp_name\n",
        "lr=0.0005\n",
        "# criterion = nn.BCELoss() #Binary case\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# # ------------Fresh training------------\n",
        "model_2_2_test = MM(output_size)\n",
        "model_2_2_test.to(device)\n",
        "# print(model_harm_p)\n",
        "optimizer = torch.optim.Adam(model_2_2_test.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "n_epochs = 5\n",
        "# early stopping patience; how long to wait after last time validation loss improved.\n",
        "patience = 25\n",
        "model_2_2_test, train_acc_list_cov, val_acc_list_cov, train_loss_list_cov, val_loss_list_cov, epoc_num = train_model(model_2_2_test, patience,\n",
        "                                                                                            n_epochs,dataloader_train_pol,\n",
        "                                                                                            dataloader_val_pol)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff7HFNRkyraC",
        "outputId": "6363b40b-8268-4da4-c037-22fc60df85c7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0, batch:0\n",
            "epoch:0, batch:5\n",
            "Validation loss decreased (inf --> 0.010531).  Saving model ...\n",
            "Epoch 1: train_loss: 0.0107 train_acc: 58.0729 | val_loss: 0.0105 val_acc: 63.2812\n",
            "epoch:1, batch:0\n",
            "epoch:1, batch:5\n",
            "Validation loss decreased (0.010531 --> 0.010369).  Saving model ...\n",
            "Epoch 2: train_loss: 0.0107 train_acc: 54.6875 | val_loss: 0.0104 val_acc: 63.2812\n",
            "epoch:2, batch:0\n",
            "epoch:2, batch:5\n",
            "Validation loss decreased (0.010369 --> 0.010284).  Saving model ...\n",
            "Epoch 3: train_loss: 0.0103 train_acc: 57.8125 | val_loss: 0.0103 val_acc: 63.2812\n",
            "epoch:3, batch:0\n",
            "epoch:3, batch:5\n",
            "EarlyStopping counter: 1 out of 25\n",
            "Epoch 4: train_loss: 0.0096 train_acc: 65.8854 | val_loss: 0.0103 val_acc: 58.5938\n",
            "epoch:4, batch:0\n",
            "epoch:4, batch:5\n",
            "Validation loss decreased (0.010284 --> 0.010067).  Saving model ...\n",
            "Epoch 5: train_loss: 0.0087 train_acc: 77.0833 | val_loss: 0.0101 val_acc: 65.6250\n",
            "Saving model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "epochs = range(n_epochs)\n",
        "# train_acc_list\n",
        "# val_acc_list\n",
        "# train_loss_list\n",
        "# val_loss_list\n",
        "# plt.plot(epochs, train_acc_list)\n",
        "# plt.plot(epochs, val_acc_list)\n",
        "def plot_train_info(train_acc_list, val_acc_list,train_loss_list, val_loss_list):\n",
        "    fig1, ax1 = plt.subplots()\n",
        "    ax1.plot(epochs, train_acc_list, label=\"train acc\")\n",
        "    ax1.plot(epochs, val_acc_list, label=\"val acc\")\n",
        "    ax1.set_title(\"accuracy plot\")\n",
        "    ax1.set_xlabel(\"epochs\")\n",
        "    ax1.legend(loc=\"upper left\")\n",
        "    fig2, ax2 = plt.subplots()\n",
        "    ax2.plot(epochs, train_loss_list, label=\"train loss\")\n",
        "    ax2.plot(epochs, val_loss_list, label=\"val loss\")\n",
        "    ax2.set_title(\"loss plot\")\n",
        "    ax2.set_xlabel(\"epochs\")\n",
        "    ax2.legend(loc=\"upper left\")"
      ],
      "metadata": {
        "id": "9cHQEOmLz8jY"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_train_info(train_acc_list_cov, val_acc_list_cov,train_loss_list_cov, val_loss_list_cov)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "Vo7X6xkuz9KM",
        "outputId": "bd8cf781-3c6d-4447-90bf-e24e8eb5ba08"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred,aa = model_evaluation(model_2_2_test, dataloader_test_pol, test_samples_frame_muliti,\"Stage 2-2_multioff\")\n",
        "ls = []\n",
        "for i in aa:\n",
        "    ls.append(i.item())\n",
        "rec = np.round(recall_score(ls, y_pred, average=\"macro\"),4)\n",
        "prec = np.round(precision_score(ls, y_pred, average=\"macro\"),4)\n",
        "f1 = np.round(f1_score(ls, y_pred, average=\"macro\"),4)\n",
        "# hl = np.round(hamming_loss(test_labels, y_pred),4)\n",
        "acc = np.round(accuracy_score(ls, y_pred),4)\n",
        "mae = np.round(mean_absolute_error(ls, y_pred),4)\n",
        "print(\"recall_score\\t: \",rec)\n",
        "print(\"precision_score\\t: \",prec)\n",
        "print(\"f1_score\\t: \",f1)\n",
        "# print(\"hamming_loss\\t: \",hl)\n",
        "# print(\"accuracy_score\\t: \",f1)\n",
        "print(classification_report(ls, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBoejnY50A4O",
        "outputId": "17c08eba-e98f-4242-e580-c45087663ae9"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall_score\t:  0.5893\n",
            "precision_score\t:  0.5905\n",
            "f1_score\t:  0.5897\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.69      0.68        77\n",
            "           1       0.51      0.49      0.50        51\n",
            "\n",
            "    accuracy                           0.61       128\n",
            "   macro avg       0.59      0.59      0.59       128\n",
            "weighted avg       0.61      0.61      0.61       128\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_best = torch.load(\"/content/drive/MyDrive/CS19-1/Week8/stage2-2best.pt\")\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6QIs3Jdlaw1",
        "outputId": "9faba3a2-3b79-4a7d-f767-001521d74afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MM(\n",
              "  (dense_vgg_1024): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "  (dense_vgg_512): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (drop20): Dropout(p=0.2, inplace=False)\n",
              "  (drop5): Dropout(p=0.05, inplace=False)\n",
              "  (dense_drob_512): Linear(in_features=768, out_features=512, bias=True)\n",
              "  (gen_key_L1): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (gen_query_L1): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (gen_key_L2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (gen_query_L2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (gen_key_L3): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (gen_query_L3): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (soft): Softmax(dim=1)\n",
              "  (soft_final): Softmax(dim=1)\n",
              "  (project_dense_512a): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (project_dense_512b): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (project_dense_512c): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (fc_out): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (sink_out): Linear(in_features=64, out_features=2, bias=True)\n",
              "  (out): Linear(in_features=256, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred,aa = model_evaluation(model_best, dataloader_test_cov, test_samples_frame_cov,\"Stage 2-2\")\n",
        "ls = []\n",
        "for i in aa:\n",
        "    ls.append(i.item())\n",
        "rec = np.round(recall_score(ls, y_pred, average=\"macro\"),4)\n",
        "prec = np.round(precision_score(ls, y_pred, average=\"macro\"),4)\n",
        "f1 = np.round(f1_score(ls, y_pred, average=\"macro\"),4)\n",
        "# hl = np.round(hamming_loss(test_labels, y_pred),4)\n",
        "acc = np.round(accuracy_score(ls, y_pred),4)\n",
        "mae = np.round(mean_absolute_error(ls, y_pred),4)\n",
        "print(\"acc_score\\t: \",acc)\n",
        "print(\"recall_score\\t: \",rec)\n",
        "print(\"precision_score\\t: \",prec)\n",
        "print(\"f1_score\\t: \",f1)\n",
        "# print(\"hamming_loss\\t: \",hl)\n",
        "# print(\"accuracy_score\\t: \",f1)\n",
        "print(classification_report(ls, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0QG8viBlfds",
        "outputId": "ebdcbc24-da32-4163-a5b6-1bde8f64804d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc_score\t:  0.825\n",
            "recall_score\t:  0.8408\n",
            "precision_score\t:  0.8236\n",
            "f1_score\t:  0.8225\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.77      0.84       196\n",
            "           1       0.72      0.91      0.80       124\n",
            "\n",
            "    accuracy                           0.82       320\n",
            "   macro avg       0.82      0.84      0.82       320\n",
            "weighted avg       0.85      0.82      0.83       320\n",
            "\n"
          ]
        }
      ]
    }
  ]
}